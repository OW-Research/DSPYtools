[
  {
    "index": 0,
    "section": "",
    "subsection": "",
    "paragraph": "%address review criteria. % The functionality envisioned is absolutely basic for any kind of data-driven research. The functionality is not tailored to a particular discipline -although extensions to address specific needs will be examined. It is centered on managing data-intensive workflows. %The two aims of the proposal are: to develop software that is responsive to scientists' needs; and to design an architecture that will allow the software to scale to national level.",
    "is_in_environment": false,
    "final": "The review criteria address the functionality that is fundamentally essential for any data-driven research. This functionality is not tailored to a specific discipline; however, extensions will be explored to meet particular needs. It focuses on managing data-intensive workflows. The two main objectives of the proposal are: to develop software that meets the needs of scientists and to design an architecture that enables the software to scale at a national level."
  },
  {
    "index": 1,
    "section": "",
    "subsection": "",
    "paragraph": "%found out from scientists about pain points -is their data in the cloud/computer/cluster? Where does their time go? Problems with data?",
    "is_in_environment": false,
    "final": "Scientists have discovered various pain points related to data management. Is their data stored in the cloud, on a computer, or within a cluster? Additionally, where do they find themselves spending most of their time? What problems are they encountering with their data?"
  },
  {
    "index": 2,
    "section": "",
    "subsection": "",
    "paragraph": "%To improve and advance science, scientists need to be able to reproduce others\u2019 data or combine data from multiple sources to learn something new.",
    "is_in_environment": false,
    "final": "To enhance and progress scientific knowledge, researchers must be able to replicate others' data or integrate data from various sources to discover new insights."
  },
  {
    "index": 3,
    "section": "Proposed Research",
    "subsection": "",
    "paragraph": "The working hypothesis of the project is that existing infrastructure (at the level of software, computation, and data) is not as widely known and used as it could and should be. Further, we hypothesize that for data, this is due to: lack of metadata that explains the semantics of the data; and lack of use of modern data formats that would facilitate data reuse. for software, this is due to problems with dependencies (i.e. when a software package requires another software package for running) and implementation issues. This in turn is due to the fact that some scientific software is developed without version control or other software engineering best practices. for computation (i.e. existing clusters and similar resources), this is due to either lack of awareness among scientists; or lack of resources (in the form of computing experts support) to make effective use of the resources. The PI and co-PI have already interviewed some people working in scientific computing, but this provides only limited evidence, so there is a need to check whether these observations generalize. One of the main tasks of this project is to test these hypotheses. If they are validated, we will build our category I proposal around the development of tools and methods to address these issues. If they are not validated, we will seek to develop alternative hypotheses about the main issues facing scientific computing in the United States nowadays.",
    "is_in_environment": true,
    "final": "The working hypothesis of the project postulates that the existing infrastructure related to software, computation, and data is not utilized to its fullest potential; that is, it is not as widely known or employed by researchers and practitioners as it logically should be. Furthermore, we propose that this underutilization of data stems from two primary factors: first, there is a significant lack of metadata which would elucidate the semantics and context of the data, making it difficult for users to understand its relevance and applications; second, there is insufficient adoption of modern data formats that could greatly enhance data reusability across different platforms and research initiatives. In regard to software, the issues are primarily attributed to dependency conflicts\u2014wherein one software package requires another in order to function properly\u2014as well as broader implementation challenges that arise during the software development lifecycle. These challenges are exacerbated by the fact that a considerable amount of scientific software is often created without the implementation of version control or adherence to other best practices in software engineering, which can lead to complications in maintenance and usability.\n\nOn the computational front, which encompasses existing clusters and similar resources, the barriers to effective usage can be categorized into two main issues: first, a lack of awareness among scientists regarding the availability and capabilities of these resources, and second, a scarcity of supporting expertise in computing resources, which limits the ability of researchers to leverage these tools effectively. The Principal Investigator (PI) and co-Principal Investigator (co-PI) have already conducted preliminary interviews with various individuals engaged in the field of scientific computing; however, this initial evidence is deemed insufficient to draw broad conclusions, highlighting the necessity for further investigation to ascertain whether these observations hold true across a wider demographic. One of the central tasks of this project is to rigorously test these hypotheses to gather more conclusive data. Should our hypotheses be validated through our research, we intend to construct our category I proposal around the creation and development of innovative tools and methods designed to address and rectify these identified issues. Conversely, if our hypotheses are not supported by the findings, we will pivot our focus towards developing alternative explanations for the primary challenges currently confronting scientific computing in the United States."
  },
  {
    "index": 4,
    "section": "Proposed Research",
    "subsection": "Key Objective",
    "paragraph": "Gather Requirements for Scalable software tools: to facilitate collaboration at large scale, any tools developed during a Category I proposal must be able to meet certain requirements, including Facilitate Data and Workflow Sharing: Ensure tools are designed to enable effective data sharing, version control, and integration between institutions, promoting collaboration and data transparency. Generate metadata, including lineage/provenance: Datasets must be annotated with descriptive information, including their origin and any preparation done (cleaning, etc.), in order to be considered for re-use by other scientists than the ones that originally created it. Workflow Management: Tools to help scientists create, track, and execute complex data analysis pipelines, including integration with popular scientific workflows and platforms. Identify Architectures that can Scale: Even if tools exist that address all identified needs, such tools need to be integrated into a coherent architecture that maximizes resource use and facilitates collaboration and coordination of multiple working groups, distributed across the whole country.",
    "is_in_environment": true,
    "final": "To gather requirements for scalable software tools that facilitate large-scale collaboration, any tools developed during a Category I proposal must meet certain criteria. First, they should facilitate data and workflow sharing, ensuring that tools are designed for effective data sharing, version control, and integration between institutions, which promotes collaboration and transparency. Second, it is essential to generate metadata, including lineage and provenance. Datasets must be annotated with descriptive information about their origin and any preparation done, such as cleaning, to be considered for reuse by scientists beyond the original creators. Third, the tools should include workflow management capabilities, helping scientists create, track, and execute complex data analysis pipelines, and integrate them with popular scientific workflows and platforms. Finally, it is crucial to identify architectures that can scale. Even if suitable tools exist to address all identified needs, they must be integrated into a coherent architecture that maximizes resource utilization and facilitates collaboration and coordination among multiple working groups distributed across the country."
  },
  {
    "index": 5,
    "section": "Proposed Research",
    "subsection": "Approach",
    "paragraph": "{Phase 1: Understanding Current Needs.} To develop the most effective tools, we must first understand the current landscape of data management in scientific research. We will conduct interviews and surveys with researchers from various fields (e.g., biology, physics, social sciences) to identify: The types of software tools, data formats, and computational resources commonly used by scientists. Pain points and limitations in existing data management systems, including issues related to data formatting, metadata generation, and sharing. This will be done in two stages: Initial surveys to gather general information about current practices in data management. Follow-up interviews to identify key challenges and bottlenecks in existing workflows and systems. We will start locally and then expand via recommendations from scientists, to ensure good response. The co-PI's institution has strong tries with Brookhaven National Laboratory, so the co-PI will approach some of their scientists and request meetings with different groups. We will interview not only the scientists but also their technical support, since they are more likely to be able to provide technical details about the computing needs of the projects they are involved in. We will also research GitHub accounts and other scientific repositories like the Journal of Open Research Software to create a repository of what is currently available to the working scientists.",
    "is_in_environment": true,
    "final": "{Phase 1: Understanding Current Needs.} To develop the most effective tools, we must first understand the current landscape of data management in scientific research. We will conduct interviews and surveys with researchers from various fields (e.g., biology, physics, social sciences) to identify: The types of software tools, data formats, and computational resources commonly used by scientists. Pain points and limitations in existing data management systems, including issues related to data formatting, metadata generation, and sharing. This will be done in two stages: Initial surveys to gather general information about current practices in data management. Follow-up interviews to identify key challenges and bottlenecks in existing workflows and systems. We will start locally and then expand via recommendations from scientists, to ensure good response. The co-PI's institution has strong tries with Brookhaven National Laboratory, so the co-PI will approach some of their scientists and request meetings with different groups. We will interview not only the scientists but also their technical support, since they are more likely to be able to provide technical details about the computing needs of the projects they are involved in. We will also research GitHub accounts and other scientific repositories like the Journal of Open Research Software to create a repository of what is currently available to the working scientists."
  }
]